{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "written by Tianpei Xie, Mar 14, 2016\n",
    "\n",
    "In this script, we analyze the accident data, convert the categorical columns into indicators, transform the features. Then we combinine the accident data with person and vehicle data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%reset \n",
    "%matplotlib inline\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sqlite3\n",
    "ifWrite = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Accident data\n",
    "from ./train  and ./test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read the accident file\n",
    "print(\"Load accident data set\")\n",
    "acc_trn_df = pd.read_csv(\"./train/accident_train.csv\")\n",
    "#acc_trn_df.fillna(0, inplace=True)\n",
    "#per_trn_df.fillna(0, inplace=True)\n",
    "#veh_trn_df.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.shape(acc_trn_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "acc_trn_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data filtering and feature hashing\n",
    "fillna, transfer categorical to numerical data, and count most frequent component\n",
    "\n",
    "### Drop irrelevant features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load the column names \n",
    "print(\"Drop irrelevant features\")\n",
    "acc_trn_org_columns = acc_trn_df.columns\n",
    "acc_drop_list = ['YEAR', 'DAY', 'MONTH','CITY','TWAY_ID', 'RAIL','NOT_HOUR',\\\n",
    "                 'NOT_MIN','ARR_HOUR','ARR_MIN','HOSP_HR','HOSP_MN']\n",
    "acc_trn_df.drop(acc_drop_list, axis = 1, inplace=True)\n",
    "acc_trn_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.shape(acc_trn_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature quantization for DAY_WEEK, HOUR and MINUTE, MILEPT\n",
    "cut DAY_WEEK to be 0 (Weekdays) or 1 (Weekends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Features Quantizations\")\n",
    "acc_trn_df['COUNTY'] = acc_trn_df['COUNTY'].apply(lambda x: x if( x < 990) else np.nan)\n",
    "acc_trn_df['DAY_WEEK'] = acc_trn_df['DAY_WEEK'].apply(lambda x: 1 if( x == 7 or x == 1 ) else 0)\n",
    "\n",
    "acc_trn_latlon_df = acc_trn_df.loc[:,['LATITUDE','LONGITUD']]\n",
    "acc_trn_latlon_df['LATITUDE'] = acc_trn_latlon_df['LATITUDE'].map(lambda x: x if(x <= 90 and x>= -90) else np.nan)\n",
    "acc_trn_latlon_df['LONGITUD'] = acc_trn_latlon_df['LONGITUD'].map(lambda x: x if(x <= 180 and x>= -180) else np.nan)\n",
    "acc_trn_df.loc[:,['LATITUDE','LONGITUD']] = acc_trn_latlon_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp_index = acc_trn_df.isnull().any(axis=1)\n",
    "acc_nan_index = temp_index.index[temp_index==True]\n",
    "len(acc_nan_index) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop NAN rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "acc_trn_dropout_df = acc_trn_df.loc[acc_nan_index,:]\n",
    "if ifWrite:\n",
    "    acc_trn_dropout_df.to_csv(\"./train/accident_train_dropout.csv\")\n",
    "acc_trn_dropout_ID = acc_trn_dropout_df['ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "acc_trn_df.dropna(axis =0, inplace=True)\n",
    "np.shape(acc_trn_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "quantization via quantile "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out_hr, bins_hr = pd.qcut(x=acc_trn_df['HOUR'], q=[0, 0.1, 0.25, 0.5, 0.75, 0.9, 1.], labels=False, retbins=True)\n",
    "acc_trn_df['HOUR'] = out_hr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "out_min, bins_min = pd.qcut(acc_trn_df['MINUTE'], q=[0, 0.1, 0.25, 0.5, 0.75, 0.9, 1.], labels=False, retbins=True)\n",
    "acc_trn_df['MINUTE'] = out_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "out_mile, bins_mile = pd.qcut(acc_trn_df['MILEPT'], q=[0, 0.1, 0.35, 0.5, 0.65, 0.8, 1.], labels=False, retbins=True)\n",
    "acc_trn_df['MILEPT'] = out_mile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "acc_trn_df.groupby(['HOUR'])['ID'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "acc_trn_df.groupby(['MILEPT'])['ID'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector quantization for (STATE, COUNTY), drop CITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp_df = acc_trn_df.loc[:,['STATE','COUNTY']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.jointplot(x='STATE', y='COUNTY',data=temp_df) #scatterplot for (STATE, COUNTY) items, see clusters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector quantization for [STATE, COUNTY]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "np.random.seed(123157) \n",
    "k_means = KMeans(n_clusters = 9, n_init= 8)\n",
    "k_means.fit(temp_df.values)\n",
    "state_county_labels = k_means.labels_\n",
    "values = k_means.cluster_centers_.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# see histogram of a subset of 5000 samples \n",
    "randIndex = np.random.choice(len(state_county_labels), 5000)\n",
    "temp = state_county_labels[randIndex]\n",
    "sns.distplot(temp, kde=False, rug=True)  # see distribution of clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "###  Extract (Latitude, Longitude) and make clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "CLustering of (Latitude, Longitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "acc_trn_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# see histogram of a subset of 5000 samples \n",
    "randIndex2 = np.random.choice(acc_trn_latlon_df.dropna(axis=0).index.tolist(), 50000)\n",
    "temp2 = acc_trn_latlon_df.loc[randIndex2,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use basemap to plot on the earth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "from mpl_toolkits.basemap import Basemap\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(30,30)\n",
    "m = Basemap(llcrnrlon=-130, llcrnrlat=np.min(temp2['LATITUDE']), \n",
    "            urcrnrlon=np.max(temp2['LONGITUD']), urcrnrlat=50, \n",
    "            #llcrnrlon=np.min(temp2['LONGITUD'])-20, llcrnrlat=np.min(temp2['LATITUDE'])-10, \\\n",
    "            #urcrnrlon=np.max(temp2['LONGITUD'])+20, urcrnrlat=np.max(temp2['LATITUDE'])+10,\\\n",
    "            projection='lcc', resolution='i', area_thresh=1, lat_1=-40, lat_2=42,lon_0=-85)\n",
    "m.drawcoastlines()\n",
    "m.drawstates()\n",
    "m.drawcountries()\n",
    "lons = temp2['LONGITUD'].values\n",
    "lats = temp2['LATITUDE'].values\n",
    "x, y = m(lons, lats)\n",
    "\n",
    "m.scatter(x, y, s=20, marker='o', color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use DBSCAN - Density-Based Spatial Clustering of Applications with Noise to cluser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_acc= len(acc_trn_df.index)\n",
    "rho = 0.2\n",
    "np.ceil(n_acc*rho).astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dbscan = DBSCAN(eps=1, min_samples = 280, algorithm='kd_tree')\n",
    "rho = 0.2\n",
    "np.random.seed(12130) \n",
    "randIndex2 = np.random.choice(acc_trn_latlon_df.dropna(axis=0).index.tolist(), np.ceil(n_acc*rho).astype(np.int64))\n",
    "subsample_trn = acc_trn_latlon_df.loc[randIndex2,:]\n",
    "dbscan.fit(X=subsample_trn.values)\n",
    "labels = dbscan.labels_\n",
    "dbscan_values = dbscan.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.distplot(labels[np.random.choice(len(labels), 1000)], kde=False, rug=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unique_labels = np.unique(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.colors as colors\n",
    "import matplotlib.cm as cmx\n",
    "jet = cm = plt.get_cmap('jet') \n",
    "cNorm  = colors.Normalize(vmin=-1, vmax=unique_labels[-1])\n",
    "scalarMap = cmx.ScalarMappable(norm=cNorm, cmap=jet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.basemap import Basemap\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(30,30)\n",
    "m = Basemap(llcrnrlon=-130, llcrnrlat=np.min(temp2['LATITUDE']), \n",
    "            urcrnrlon=np.max(temp2['LONGITUD']), urcrnrlat=50, \n",
    "            #llcrnrlon=np.min(temp2['LONGITUD'])-20, llcrnrlat=np.min(temp2['LATITUDE'])-10, \\\n",
    "            #urcrnrlon=np.max(temp2['LONGITUD'])+20, urcrnrlat=np.max(temp2['LATITUDE'])+10,\\\n",
    "            projection='lcc', resolution='i', area_thresh=1, lat_1=-40, lat_2=42,lon_0=-85)\n",
    "m.drawcoastlines()\n",
    "m.drawstates()\n",
    "m.drawcountries()\n",
    "for c in np.unique(labels):\n",
    "    lons = acc_trn_latlon_df.loc[randIndex2[labels==c],'LONGITUD'].values\n",
    "    lats = acc_trn_latlon_df.loc[randIndex2[labels==c],'LATITUDE'].values\n",
    "    x, y = m(lons, lats)\n",
    "    colorVal = scalarMap.to_rgba(c)\n",
    "    m.scatter(x, y, s=20, marker='o', color=colorVal, label=str(c))\n",
    "\n",
    "#ax = plt.gca()\n",
    "#handles, labels = ax.get_legend_handles_labels()  \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Predict cluster labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_batch = 10000\n",
    "label_temp = np.zeros((n_acc,),dtype=np.int64)\n",
    "np.ceil(n_acc/n_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predict labels for all training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in np.arange(np.ceil(n_acc/n_batch).astype(np.int64), dtype=np.int64):\n",
    "    if i != np.ceil(n_acc/n_batch)-1:\n",
    "        index_set = acc_trn_df.index[np.arange(start=i*n_batch, stop=(i+1)*n_batch)]\n",
    "        temp_val = acc_trn_df.loc[index_set,['LATITUDE','LONGITUD']].values\n",
    "        label_temp[i*n_batch:(i+1)*n_batch] = dbscan.fit_predict(temp_val)\n",
    "    else:\n",
    "        index_set = acc_trn_df.index[np.arange(start=i*n_batch, stop=n_acc)]\n",
    "        temp_val = acc_trn_df.loc[index_set,['LATITUDE','LONGITUD']].values\n",
    "        label_temp[i*n_batch:n_acc] = dbscan.fit_predict(temp_val)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "array_temp = np.column_stack((acc_trn_df['ID'].astype(np.int64).tolist(),label_temp))   \n",
    "acc_trn_lat_lon_label_df = pd.DataFrame(data=array_temp, columns=[\"ID\",\"LAT_LON_CLUSTER\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save a series\n",
    "array_temp = np.column_stack((acc_trn_df['ID'].astype(np.int64).tolist(),state_county_labels))   \n",
    "acc_trn_state_county_df= pd.DataFrame(data=array_temp, columns=[\"ID\", \"STATE_COUNTY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "acc_trn_lat_lon_label_df['LAT_LON_CLUSTER'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save for clusters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if ifWrite:\n",
    "    acc_trn_lat_lon_label_df.to_csv('./lat_lon_label.csv')\n",
    "    acc_trn_state_county_df.to_csv('./state_county_label.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert HOUR, MINUTE, WEEK_DAYS, (STATE, COUNTY) into indicators,  merge dataframe\n",
    "merge with (STATE, COUNTY) cluster labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "acc_trn_df_ext = acc_trn_df.merge(right=acc_trn_state_county_df, how='left', on='ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "merge with (LATITUDE, LONGITUD) cluster labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "acc_trn_df_ext = acc_trn_df_ext.merge(right=acc_trn_lat_lon_label_df, how='left', on='ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "drop (STATE, COUNTY, LATITUDE, LONGITUD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "acc_trn_df_ext.drop(['STATE' , 'COUNTY'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.shape(acc_trn_df_ext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get dummy variables for the following list of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"get dummy variables for the following list of features\")\n",
    "cat_list = ['DAY_WEEK','HOUR', 'MINUTE', 'MILEPT','ROAD_FNC','ROUTE','SP_JUR',\\\n",
    "             'MAN_COLL','REL_ROAD','LGT_COND','WEATHER', \\\n",
    "             'SCH_BUS']\n",
    "add_dummies = []\n",
    "column_sel = 'HOUR'\n",
    "for i,column_sel in enumerate(cat_list): \n",
    "    add_dummies = []\n",
    "    temp_dummy  = pd.get_dummies(acc_trn_df_ext[column_sel]).astype(np.int64)\n",
    "    column_name = ['ID']\n",
    "    add_dummies  = np.column_stack((acc_trn_df['ID'].astype(np.int64).tolist(), \\\n",
    "                              temp_dummy.values))\n",
    "    \n",
    "    for cc in temp_dummy.columns:\n",
    "        column_name.append(column_sel+'_'+str(cc))\n",
    "\n",
    "    acc_trn_df_ext = acc_trn_df_ext.merge(right=pd.DataFrame(data=add_dummies, columns=column_name), \\\n",
    "                                      how='left', on='ID')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "acc_trn_df_ext.drop(cat_list, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.shape(acc_trn_df_ext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#acc_trn_df_ext.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform the test samples\n",
    "Following the same step as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Load test data: \")\n",
    "#Load data \n",
    "acc_tst_df  = pd.read_csv(\"./test/accident_test.csv\")\n",
    "\n",
    "#fillna\n",
    "acc_tst_df.fillna(0, inplace=True)\n",
    "\n",
    "print(\"Feature filtering and quanitization...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "acc_drop_list = ['DAY', 'MONTH','CITY','TWAY_ID', 'RAIL','NOT_HOUR',\\\n",
    "                 'NOT_MIN','ARR_HOUR','ARR_MIN','HOSP_HR','HOSP_MN']\n",
    "acc_tst_df.drop(acc_drop_list, axis = 1, inplace=True)\n",
    "acc_tst_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training set\n",
    "\n",
    "Index([u'ID', u'STATE', u'VE_FORMS', u'PEDS', u'PERSONS', u'COUNTY',\n",
    "       u'DAY_WEEK', u'HOUR', u'MINUTE', u'NHS', u'ROAD_FNC', u'ROUTE',\n",
    "       u'MILEPT', u'LATITUDE', u'LONGITUD', u'SP_JUR', u'HARM_EV', u'MAN_COLL',\n",
    "       u'REL_ROAD', u'LGT_COND', u'WEATHER', u'SCH_BUS', u'CF1', u'CF2',\n",
    "       u'CF3', u'FATALS', u'DRUNK_DR'],\n",
    "      dtype='object')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training set (287586, 27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.shape(acc_tst_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#acc_tst_df['COUNTY'] = acc_tst_df['COUNTY'].apply(lambda x: x if( x < 990) else np.nan)\n",
    "acc_tst_df['DAY_WEEK'] = acc_tst_df['DAY_WEEK'].apply(lambda x: 1 if( x == 7 or x == 1 ) else 0)\n",
    "\n",
    "acc_tst_latlon_df = acc_tst_df.loc[:,['LATITUDE','LONGITUD']]\n",
    "#acc_tst_latlon_df['LATITUDE'] = acc_tst_latlon_df['LATITUDE'].map(lambda x: x if(x <= 90 and x>= -90) else np.nan)\n",
    "#acc_tst_latlon_df['LONGITUD'] = acc_tst_latlon_df['LONGITUD'].map(lambda x: x if(x <= 180 and x>= -180) else np.nan)\n",
    "#acc_tst_df.loc[:,['LATITUDE','LONGITUD']] = acc_tst_latlon_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.shape(acc_tst_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "out_hr_tst = pd.cut(x=acc_tst_df['HOUR'], bins=bins_hr, labels=False)\n",
    "acc_tst_df['HOUR'] = out_hr_tst.fillna(value=0,axis=0).astype(np.int64)\n",
    "\n",
    "out_min_tst = pd.cut(x=acc_tst_df['MINUTE'], bins=bins_min, labels=False)\n",
    "acc_tst_df['MINUTE'] = out_min_tst.fillna(value=0,axis=0).astype(np.int64)\n",
    "\n",
    "out_mile_tst = pd.cut(x=acc_tst_df['MILEPT'], bins=bins_mile, labels=False)\n",
    "acc_tst_df['MILEPT'] = out_mile_tst.fillna(value=0,axis=0).astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tst_state_county_labels = k_means.fit_predict(acc_tst_df.loc[:,['STATE','COUNTY']].values)\n",
    "tst_label = dbscan.fit_predict(acc_tst_df.loc[:,['LATITUDE','LONGITUD']].values)\n",
    "unique, counts = np.unique(tst_label, return_counts=True)\n",
    "print(np.asarray((unique, counts)).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "array_temp_tst = np.column_stack((acc_tst_df['ID'].astype(np.int64).tolist(), tst_label))\n",
    "acc_tst_lat_lon_label_df = pd.DataFrame(data=array_temp_tst, columns=[\"ID\",\"LAT_LON_CLUSTER\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "array_temp_tst = np.column_stack((acc_tst_df['ID'].astype(np.int64).tolist(), tst_state_county_labels))\n",
    "acc_tst_state_county_df= pd.DataFrame(data=array_temp_tst, columns=[\"ID\", \"STATE_COUNTY\"])\n",
    "%xdel array_temp_tst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "acc_tst_df_ext = acc_tst_df.merge(right=acc_tst_state_county_df, how='left', on='ID')\n",
    "acc_tst_df_ext = acc_tst_df_ext.merge(right=acc_tst_lat_lon_label_df, how='left', on='ID')\n",
    "acc_tst_df_ext.drop(['STATE' , 'COUNTY'], axis=1, inplace=True)\n",
    "np.shape(acc_tst_df_ext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training (277996, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cat_list = ['DAY_WEEK','HOUR', 'MINUTE', 'MILEPT','ROAD_FNC','ROUTE','SP_JUR',\\\n",
    "             'MAN_COLL','REL_ROAD','LGT_COND','WEATHER', \\\n",
    "             'SCH_BUS']\n",
    "add_dummies = []\n",
    "column_sel = 'HOUR'\n",
    "for i,column_sel in enumerate(cat_list): \n",
    "    add_dummies = []\n",
    "    temp_dummy  = pd.get_dummies(acc_tst_df_ext[column_sel]).astype(np.int64)\n",
    "    column_name = ['ID']\n",
    "    add_dummies  = np.column_stack((acc_tst_df['ID'].astype(np.int64).tolist(), \\\n",
    "                              temp_dummy.values))\n",
    "    for cc in temp_dummy.columns:\n",
    "        column_name.append(column_sel+'_'+str(cc))\n",
    "\n",
    "    acc_tst_df_ext = acc_tst_df_ext.merge(right=pd.DataFrame(data=add_dummies, columns=column_name), \\\n",
    "                                      how='left', on='ID')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "acc_tst_df_ext.drop(cat_list, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.shape(acc_tst_df_ext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training set (277996, 115)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "acc_tst_df_ext.columns[~acc_tst_df_ext.columns.isin(acc_trn_df_ext.columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "acc_trn_df_ext.columns[~acc_trn_df_ext.columns.isin(acc_tst_df_ext.columns)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "combine and expand the training and test features to make them consistent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_acc_trn = acc_trn_df_ext.shape[0]\n",
    "n_acc_tst = acc_tst_df_ext.shape[0]\n",
    "temp4 = acc_trn_df_ext.merge(right=acc_tst_df_ext, how='outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extract extended training and test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "acc_trn_df_ext = temp4.iloc[np.arange(start=0,stop=n_acc_trn),:].fillna(0,axis=1)\n",
    "\n",
    "acc_tst_df_ext = temp4.iloc[np.arange(start=n_acc_trn,stop=n_acc_trn+n_acc_tst),:].drop('DRUNK_DR',axis=1).fillna(0,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%xdel temp4\n",
    "n_acc_trn = acc_trn_df_ext.shape[0]\n",
    "acc_trn_df_ext.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "acc_tst_df_ext.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save the accident data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Ydata = acc_trn_df_ext['DRUNK_DR'].values\n",
    "Ydata_df = acc_trn_df_ext.loc[:, ['ID','DRUNK_DR']]\n",
    "\n",
    "acc_trn_df_ext.drop('DRUNK_DR',axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if ifWrite:\n",
    "    print(\"save to files for both training and testing data ...\")\n",
    "    try:\n",
    "        os.remove('./train/accident_train_ext.csv')\n",
    "        os.remove('./test/accident_test_ext.csv')\n",
    "    except OSError:\n",
    "        pass\n",
    "    acc_trn_df_ext.to_csv('./train/accident_train_ext.csv')\n",
    "    Ydata_df.to_csv('./train/labels_ext.csv')\n",
    "    acc_tst_df_ext.to_csv('./test/accident_test_ext.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if ifWrite:\n",
    "    print(\"save to sql database for training set  ...\")\n",
    "    conn = sqlite3.connect(\"./train/joint_accident_person_vehicle.db\")\n",
    "    n_batch = 1000\n",
    "\n",
    "    for i in np.arange(np.ceil(n_acc_trn/n_batch).astype(np.int64), dtype=np.int64):\n",
    "        if i == 0:\n",
    "            temp = pd.read_csv('./train/accident_train_ext.csv', nrows=n_batch, index_col=0)\n",
    "            temp.to_sql(\"accident_trans\", conn, if_exists= \"replace\", index=True)\n",
    "        else:\n",
    "            if i!= np.ceil(n_per_veh/n_batch).astype(np.int64)-1:\n",
    "                temp = pd.read_csv('./train/accident_train_ext.csv', \\\n",
    "                               skiprows= range(1, i*n_batch+1),\\\n",
    "                               nrows=n_batch, index_col=0)\n",
    "                temp.to_sql(\"accident_trans\", conn, if_exists= \"append\", index=True)\n",
    "            else:\n",
    "                temp = pd.read_csv('./train/accident_train_ext.csv', \\\n",
    "                               skiprows= range(1, i*n_batch+1),\\\n",
    "                               nrows=n_batch, index_col=0)\n",
    "                temp.to_sql(\"accident_trans\", conn, if_exists= \"append\", index=True)\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%xdel  acc_trn_df\n",
    "%xdel  acc_tst_df\n",
    "acc_trn_df_ext.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%xdel acc_trn_lat_lon_label_df\n",
    "%xdel acc_trn_state_county_df\n",
    "%xdel acc_tst_latlon_df\n",
    "%xdel acc_tst_state_county_df\n",
    "%xdel acc_tst_lat_lon_label_df\n",
    "%xdel label_temp\n",
    "%xdel tst_state_county_labels\n",
    "%xdel state_county_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Person dataset \n",
    "\n",
    "We merge from person dataset. Note that more than one person is involved in the accident, they will span multiple rows.\n",
    "\n",
    "We now work on person data itself\n",
    "\n",
    "First, we delete duplicates columns in person dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read the person file\n",
    "per_trn_df = pd.read_csv(\"./train/person_train.csv\")\n",
    "# read the test file\n",
    "per_tst_df = pd.read_csv(\"./test/person_test.csv\")\n",
    "per_tst_df.fillna(0, inplace=True)\n",
    "per_trn_org_columns = per_trn_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "duplicate_list = []\n",
    "per_trn_org_columns[per_trn_org_columns.isin(acc_trn_org_columns)].drop('ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "duplicate_list = per_trn_org_columns[per_trn_org_columns.isin(acc_trn_org_columns)].drop('ID')\n",
    "per_trn_df_ext = per_trn_df.drop(duplicate_list, axis=1)\n",
    "per_tst_df_ext = per_tst_df.drop(duplicate_list, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "irrelevant_cols = ['CERT_NO','EMER_USE','AIR_BAG','DEATH_MO','DEATH_DA','DEATH_HR','DEATH_MN','DEATH_TM',\\\n",
    "                   'EJ_PATH', 'EXTRICAT','ROLLOVER','LAG_HRS','LAG_MINS','FIRE_EXP']\n",
    "per_trn_df_ext.drop(irrelevant_cols, axis=1, inplace=True)\n",
    "per_tst_df_ext.drop(irrelevant_cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "per_trn_df_ext.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete the rows that correspond to the deleted rows in accident data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "per_nan_index = per_trn_df_ext.index[per_trn_df_ext['ID'].isin(acc_trn_dropout_ID)]\n",
    "per_trn_dropout_df = per_trn_df_ext.loc[per_nan_index,:]\n",
    "if ifWrite:\n",
    "    per_trn_dropout_df.to_csv(\"./train/person_train_dropout.csv\")\n",
    "per_trn_df_ext.drop(per_nan_index, axis=0)\n",
    "per_trn_df_ext.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantization for AGE,  PER_TYP\n",
    "\n",
    "Note (PER_NO, VEH_NO) associates each person with a vehicle. For example, ID=77, has 4 people involved \n",
    "given as \n",
    "\\begin{table}[ht]\n",
    "  \\begin{center}\n",
    "    \\begin{tabular}{cc}\n",
    "      \\toprule\n",
    "        PER_NO & VEH_NO \\\\\n",
    "           $1$   &   $1$\\\\    \n",
    "           $1$   &   $2$\\\\    \n",
    "           $2$   &   $2$\\\\    \n",
    "           $3$   &   $2$\\\\    \n",
    "      \\bottomrule\n",
    "    \\end{tabular}\n",
    "  \\end{center}\n",
    "\\end{table}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out_age, bins_age = pd.qcut(x=per_trn_df_ext['AGE'], q=[0, 0.1, 0.25, 0.5, 0.75, 0.9, 1.], labels=False, retbins=True)\n",
    "per_trn_df_ext['AGE'] = out_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#out_per, bins_per = pd.qcut(x=per_trn_df_ext['PER_NO'], q=[0, 0.75, 0.9, 1.], labels=False, retbins=True)\n",
    "#per_trn_df_ext['PER_NO'] = out_per"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#out_veh, bins_veh = pd.qcut(x=per_trn_df_ext['VEH_NO'], q=[0, 0.5, 0.9, 1.], labels=False, retbins=True)\n",
    "#per_trn_df_ext['VEH_NO'] = out_veh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "per_trn_df_ext['PER_TYP'] = per_trn_df_ext['PER_TYP'].apply(lambda x: x if (x not in [-1, 5,6,7,8,19]) else 5 )\n",
    "per_tst_df_ext['PER_TYP'] = per_tst_df_ext['PER_TYP'].apply(lambda x: x if (x not in [-1, 5,6,7,8,19]) else 5 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out_age_tst = pd.cut(x=per_tst_df_ext['AGE'], bins=bins_age, labels=False)\n",
    "per_tst_df_ext['AGE'] = out_age_tst.fillna(value=0,axis=0).astype(np.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get dummy variables for categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "per_trn_df_ext.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp_index_per = per_trn_df_ext.index.tolist()\n",
    "per_trn_df_ext['EXTRA_INDEX'] = per_trn_df_ext.index.tolist() #the ID is duplicate\n",
    "cat_list_per = ['AGE','SEX', 'PER_TYP', 'SPEC_USE' ,'SEAT_POS','DOA','HISPANIC','RACE', 'IMPACT1',\\\n",
    "                'WORK_INJ', 'INJ_SEV']\n",
    "add_dummies = []\n",
    "for column_sel in cat_list_per: \n",
    "    add_dummies = []\n",
    "    temp_dummy  = pd.get_dummies(per_trn_df_ext[column_sel]).astype(np.int64)\n",
    "    column_name = ['EXTRA_INDEX']\n",
    "    add_dummies  = np.column_stack((per_trn_df_ext.index.tolist(), \\\n",
    "                              temp_dummy.values))\n",
    "    for cc in temp_dummy.columns:\n",
    "        column_name.append(column_sel+'_'+str(cc))\n",
    "\n",
    "    per_trn_df_ext = per_trn_df_ext.merge(right=pd.DataFrame(data=add_dummies, columns=column_name), \\\n",
    "                                      how='left', on='EXTRA_INDEX')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "per_trn_df_ext.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete redundant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#per_trn_df_ext['EXTRA_INDEX']\n",
    "#%xdel  acc_trn_df,  acc_tst_df\n",
    "n_per_trn = per_trn_df_ext.shape[0]\n",
    "if ifWrite:\n",
    "    try:\n",
    "        os.remove('./train/person_temp.csv')\n",
    "    except OSError:\n",
    "        pass\n",
    "    per_trn_df_ext.to_csv('./train/person_temp.csv')\n",
    "%xdel per_trn_df_ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_batch = 10000\n",
    "for i in np.arange(np.ceil(n_per_trn/n_batch).astype(np.int64), dtype=np.int64):\n",
    "    if i == 0:\n",
    "        temp = pd.read_csv('./train/person_temp.csv', nrows=n_batch, index_col=0)\n",
    "        temp.drop('EXTRA_INDEX',axis=1,inplace=True)\n",
    "        temp.drop(cat_list_per,axis=1,inplace=True)\n",
    "        per_trn_df_ext = temp\n",
    "    else:\n",
    "        temp = pd.read_csv('./train/person_temp.csv', \\\n",
    "                               skiprows= range(1, i*n_batch+1),\\\n",
    "                               nrows=n_batch, index_col=0)\n",
    "        temp.drop('EXTRA_INDEX',axis=1,inplace=True)\n",
    "        temp.drop(cat_list_per,axis=1,inplace=True)\n",
    "        per_trn_df_ext = pd.concat([per_trn_df_ext, temp])\n",
    "#per_trn_df_ext.index = temp_index_per"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "per_trn_df_ext.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tranform the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp_index_per_tst = per_tst_df_ext.index.tolist()\n",
    "per_tst_df_ext['EXTRA_INDEX'] = per_tst_df_ext.index.tolist() #the ID is duplicate\n",
    "cat_list_per = ['AGE','SEX', 'PER_TYP', 'SPEC_USE' ,'SEAT_POS','DOA','HISPANIC','RACE', 'IMPACT1',\\\n",
    "                'WORK_INJ', 'INJ_SEV']\n",
    "add_dummies = []\n",
    "for column_sel in cat_list_per: \n",
    "    add_dummies = []\n",
    "    temp_dummy  = pd.get_dummies(per_tst_df_ext[column_sel]).astype(np.int64)\n",
    "    column_name = ['EXTRA_INDEX']\n",
    "    add_dummies  = np.column_stack((per_tst_df_ext.index.tolist(), \\\n",
    "                              temp_dummy.values))\n",
    "    for cc in temp_dummy.columns:\n",
    "        column_name.append(column_sel+'_'+str(cc))\n",
    "\n",
    "    per_tst_df_ext = per_tst_df_ext.merge(right=pd.DataFrame(data=add_dummies, columns=column_name), \\\n",
    "                                      how='left', on='EXTRA_INDEX')  \n",
    "per_tst_df_ext.shape    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_per_tst = per_tst_df_ext.shape[0]\n",
    "per_tst_df_ext.drop('EXTRA_INDEX',axis=1,inplace=True)\n",
    "per_tst_df_ext.drop(cat_list_per,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_trn_trail = 2; n_tst_trial = 2\n",
    "temp_tst = per_tst_df_ext.loc[range(0,n_tst_trial),:]\n",
    "temp_trn = per_trn_df_ext.loc[range(n_per_trn-n_trn_trail,n_per_trn),:]\n",
    "\n",
    "temp = temp_trn.merge(right=temp_tst, how='outer')\n",
    "temp4 =  per_trn_df_ext.merge(right=temp, how='outer')\n",
    "#temp4 = per_trn_df_ext.merge(right=per_tst_df_ext, how='outer')\n",
    "per_trn_df_ext = temp4.iloc[np.arange(start=0,stop=n_per_trn),:].fillna(0,axis=1)\n",
    "%xdel temp4\n",
    "\n",
    "temp4 =  temp.merge(right=per_tst_df_ext, how='outer')\n",
    "per_tst_df_ext = temp4.iloc[np.arange(start=n_trn_trail,stop=temp4.shape[0]),:].fillna(0,axis=1)\n",
    "per_tst_df_ext.index = temp_index_per_tst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%xdel temp4\n",
    "per_trn_df_ext.shape\n",
    "#%xdel per_trn_df_ext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(732461, 143)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "per_tst_df_ext.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if ifWrite:\n",
    "    try:\n",
    "        os.remove('./train/person_train_ext.csv')\n",
    "        os.remove('./test/person_test_ext.csv')\n",
    "    except OSError:\n",
    "        pass\n",
    "    per_trn_df_ext.to_csv('./train/person_train_ext.csv')\n",
    "    per_tst_df_ext.to_csv('./test/person_test_ext.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%xdel per_trn_df_ext\n",
    "%xdel per_tst_df_ext\n",
    "%xdel temp_index_per_tst\n",
    "%xdel temp_index_per\n",
    "%xdel temp\n",
    "%xdel add_nummies\n",
    "%xdel temp_dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if ifWrite:\n",
    "    conn = sqlite3.connect(\"./train/joint_accident_person_vehicle.db\")\n",
    "    n_batch = 1000\n",
    "\n",
    "    for i in np.arange(np.ceil(n_per_trn/n_batch).astype(np.int64), dtype=np.int64):\n",
    "        if i == 0:\n",
    "            temp = pd.read_csv('./train/person_train_ext.csv', nrows=n_batch, index_col=0)\n",
    "            temp.to_sql(\"person_trans\", conn, if_exists= \"replace\", index=True)\n",
    "        else:\n",
    "            if i!= np.ceil(n_per_veh/n_batch).astype(np.int64)-1:\n",
    "                temp = pd.read_csv('./train/person_train_ext.csv', \\\n",
    "                               skiprows= range(1, i*n_batch+1),\\\n",
    "                               nrows=n_batch, index_col=0)\n",
    "                temp.to_sql(\"person_trans\", conn, if_exists= \"append\", index=True)\n",
    "            else:\n",
    "                temp = pd.read_csv('./train/person_train_ext.csv', \\\n",
    "                               skiprows= range(1, i*n_batch+1),\\\n",
    "                               nrows=n_batch, index_col=0)\n",
    "                temp.to_sql(\"person_trans\", conn, if_exists= \"append\", index=True)\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vehicle data\n",
    "Process the individul vehicle data\n",
    "\n",
    "First, remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read the vehicle file\n",
    "veh_trn_df = pd.read_csv(\"./train/vehicle_train.csv\")\n",
    "veh_tst_df = pd.read_csv(\"./test/vehicle_test.csv\")\n",
    "veh_tst_df.fillna(0, inplace=True)\n",
    "veh_trn_org_columns = veh_trn_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "veh_trn_org_columns[veh_trn_org_columns.isin(acc_trn_org_columns)].drop('ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "duplicate_list = veh_trn_org_columns[veh_trn_org_columns.isin(acc_trn_org_columns)].drop('ID')\n",
    "veh_trn_df_ext = veh_trn_df.drop(duplicate_list, axis=1)\n",
    "veh_tst_df_ext = veh_tst_df.drop(duplicate_list, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete rows that corresponds to deleted ID in accident data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "veh_nan_index = veh_trn_df_ext.index[veh_trn_df_ext['ID'].isin(acc_trn_dropout_ID)]\n",
    "veh_trn_dropout_df = veh_trn_df_ext.loc[veh_nan_index,:]\n",
    "if ifWrite:\n",
    "    veh_trn_dropout_df.to_csv(\"./train/vehicle_train_dropout.csv\")\n",
    "veh_trn_df_ext.drop(veh_nan_index, axis=0)\n",
    "veh_trn_df_ext.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "irrelevant_cols = ['BUS_USE','DR_ZIP','VIN','VIN_1','VIN_2','VIN_3','VIN_4',\\\n",
    "                   'VIN_5','VIN_6','VIN_7','VIN_8','VIN_9',\\\n",
    "                   'VIN_10','VIN_11','VIN_12', 'TOW_VEH','ROLLOVER', 'DEFORMED', \\\n",
    "                   'MCARR_ID','EMER_USE','CARGO_BT','DEATHS','FIRE_EXP','UNDERIDE','LAST_YR','FIRST_YR']\n",
    "veh_trn_df_ext.drop(irrelevant_cols, axis=1, inplace=True)\n",
    "veh_tst_df_ext.drop(irrelevant_cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out_speed, bins_speed = pd.qcut(x=veh_trn_df_ext['TRAV_SP'], q=[0, 0.1, 0.25, 0.5, 0.75, 0.9, 1.], labels=False, retbins=True)\n",
    "veh_trn_df_ext['TRAV_SP'] = out_speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out_speed_tst = pd.cut(x=veh_tst_df_ext['TRAV_SP'], bins=bins_speed, labels=False)\n",
    "veh_tst_df_ext['TRAV_SP'] = out_speed_tst.fillna(value=0,axis=0).astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "out_pre_acc, bins_pre_acc = pd.qcut(x=veh_trn_df_ext['PREV_ACC'], q=[0, 0.8, 0.9, 1.], labels=False, retbins=True)\n",
    "veh_trn_df_ext['PREV_ACC'] = out_pre_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out_pre_acc_tst = pd.cut(x=veh_tst_df_ext['PREV_ACC'], bins=bins_pre_acc, labels=False)\n",
    "veh_tst_df_ext['PREV_ACC'] = out_pre_acc_tst.fillna(value=0,axis=0).astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "out_pre_sus, bins_pre_sus = pd.qcut(x=veh_trn_df_ext['PREV_SUS'], q=[0, 0.9, 0.95, 1.], labels=False, retbins=True)\n",
    "veh_trn_df_ext['PREV_SUS'] = out_pre_sus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out_pre_sus_tst = pd.cut(x=veh_tst_df_ext['PREV_SUS'], bins=bins_pre_sus, labels=False)\n",
    "veh_tst_df_ext['PREV_SUS'] = out_pre_sus_tst.fillna(value=0,axis=0).astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "out_pre_dwi, bins_pre_dwi = pd.qcut(x=veh_trn_df_ext['PREV_DWI'], q=[0, 0.95, 1.], labels=False, retbins=True)\n",
    "veh_trn_df_ext['PREV_DWI'] = out_pre_dwi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out_pre_dwi_tst = pd.cut(x=veh_tst_df_ext['PREV_DWI'], bins=bins_pre_dwi, labels=False)\n",
    "veh_tst_df_ext['PREV_DWI'] = out_pre_dwi_tst.fillna(value=0,axis=0).astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "out_pre_speed, bins_pre_speed = pd.qcut(x=veh_trn_df_ext['PREV_SPD'], q=[0, 0.9, 0.95, 1.], labels=False, retbins=True)\n",
    "veh_trn_df_ext['PREV_SPD'] = out_pre_speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out_pre_speed_tst = pd.cut(x=veh_tst_df_ext['PREV_SPD'], bins=bins_pre_speed, labels=False)\n",
    "veh_tst_df_ext['PREV_SPD'] = out_pre_speed_tst.fillna(value=0,axis=0).astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "out_pre_other, bins_pre_other = pd.qcut(x=veh_trn_df_ext['PREV_OTH'], q=[0, 0.9, 0.95, 1.], labels=False, retbins=True)\n",
    "veh_trn_df_ext['PREV_OTH'] = out_pre_other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out_pre_other_tst = pd.cut(x=veh_tst_df_ext['PREV_OTH'], bins=bins_pre_other, labels=False)\n",
    "veh_tst_df_ext['PREV_OTH'] = out_pre_other_tst.fillna(value=0,axis=0).astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "veh_trn_df_ext.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp_index_veh = veh_trn_df_ext.index.tolist()\n",
    "veh_trn_df_ext['EXTRA_INDEX'] = veh_trn_df_ext.index.tolist() #the ID is duplicate\n",
    "cat_list_per = ['OWNER', 'J_KNIFE', 'SPEC_USE', 'TRAV_SP', 'IMPACT1', 'M_HARM' ,'PREV_ACC', 'PREV_SUS' , \\\n",
    "                'PREV_DWI' , 'PREV_SPD' ,'PREV_OTH', 'L_STATUS', 'CDL_STAT', 'L_ENDORS', 'L_COMPL',\\\n",
    "                'L_RESTRI', 'LAST_MO']\n",
    "add_dummies = []\n",
    "for column_sel in cat_list_per: \n",
    "    add_dummies = []\n",
    "    temp_dummy  = pd.get_dummies(veh_trn_df_ext[column_sel]).astype(np.int64)\n",
    "    column_name = ['EXTRA_INDEX']\n",
    "    add_dummies  = np.column_stack((veh_trn_df_ext.index.tolist(), \\\n",
    "                              temp_dummy.values))\n",
    "    for cc in temp_dummy.columns:\n",
    "        column_name.append(column_sel+'_'+str(cc))\n",
    "\n",
    "    veh_trn_df_ext = veh_trn_df_ext.merge(right=pd.DataFrame(data=add_dummies, columns=column_name), \\\n",
    "                                      how='left', on='EXTRA_INDEX')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "veh_trn_df_ext.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if ifWrite:\n",
    "    try:\n",
    "        os.remove('./train/vehicle_temp.csv')\n",
    "    except OSError:\n",
    "        pass\n",
    "\n",
    "    n_veh_trn = veh_trn_df_ext.shape[0]\n",
    "    veh_trn_df_ext.to_csv('./train/vehicle_temp.csv')\n",
    "%xdel veh_trn_df_ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_batch = 10000\n",
    "for i in np.arange(np.ceil(n_veh_trn/n_batch).astype(np.int64), dtype=np.int64):\n",
    "    if i == 0:\n",
    "        temp = pd.read_csv('./train/vehicle_temp.csv', nrows=n_batch, index_col=0)\n",
    "        temp.drop('EXTRA_INDEX',axis=1,inplace=True)\n",
    "        temp.drop(cat_list_per,axis=1,inplace=True)\n",
    "        veh_trn_df_ext = temp\n",
    "    else:\n",
    "        temp = pd.read_csv('./train/vehicle_temp.csv', \\\n",
    "                               skiprows= range(1, i*n_batch+1),\\\n",
    "                               nrows=n_batch, index_col=0)\n",
    "        temp.drop('EXTRA_INDEX',axis=1,inplace=True)\n",
    "        temp.drop(cat_list_per,axis=1,inplace=True)\n",
    "        veh_trn_df_ext = pd.concat([veh_trn_df_ext, temp])\n",
    "#veh_trn_df_ext.index = temp_index_veh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "veh_trn_df_ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#veh_trn_df_ext.drop('EXTRA_INDEX',axis=1,inplace=True)\n",
    "#veh_trn_df_ext.drop(cat_list_per,axis=1,inplace=True)\n",
    "veh_trn_df_ext.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp_index_veh_tst =  veh_tst_df_ext.index.tolist() \n",
    "veh_tst_df_ext['EXTRA_INDEX'] = veh_tst_df_ext.index.tolist() #the ID is duplicate\n",
    "cat_list_per = ['OWNER', 'J_KNIFE', 'SPEC_USE', 'TRAV_SP', 'IMPACT1', 'M_HARM' ,'PREV_ACC', 'PREV_SUS' , \\\n",
    "                'PREV_DWI' , 'PREV_SPD' ,'PREV_OTH', 'L_STATUS', 'CDL_STAT', 'L_ENDORS', 'L_COMPL',\\\n",
    "                'L_RESTRI', 'LAST_MO']\n",
    "add_dummies = []\n",
    "for column_sel in cat_list_per: \n",
    "    add_dummies = []\n",
    "    temp_dummy  = pd.get_dummies(veh_tst_df_ext[column_sel]).astype(np.int64)\n",
    "    column_name = ['EXTRA_INDEX']\n",
    "    add_dummies  = np.column_stack((veh_tst_df_ext.index.tolist(), \\\n",
    "                              temp_dummy.values))\n",
    "    for cc in temp_dummy.columns:\n",
    "        column_name.append(column_sel+'_'+str(cc))\n",
    "\n",
    "    veh_tst_df_ext = veh_tst_df_ext.merge(right=pd.DataFrame(data=add_dummies, columns=column_name), \\\n",
    "                                      how='left', on='EXTRA_INDEX')  \n",
    "veh_tst_df_ext.shape    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%xdel add_dummies\n",
    "n_veh_tst = veh_tst_df_ext.shape[0]\n",
    "veh_tst_df_ext.drop('EXTRA_INDEX',axis=1,inplace=True)\n",
    "veh_tst_df_ext.drop(cat_list_per,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_trn_trail = 2; n_tst_trial = 2\n",
    "temp_tst = veh_tst_df_ext.loc[np.arange(start=0,stop=n_tst_trial),:]\n",
    "temp_trn = veh_trn_df_ext.loc[np.arange(start=n_veh_trn-n_trn_trail,stop=n_veh_trn),:]\n",
    "\n",
    "temp = temp_trn.merge(right=temp_tst, how='outer')\n",
    "temp4 =  veh_trn_df_ext.merge(right=temp, how='outer')\n",
    "#temp4 = per_trn_df_ext.merge(right=per_tst_df_ext, how='outer')\n",
    "veh_trn_df_ext = temp4.iloc[np.arange(start=0,stop=n_veh_trn),:].fillna(0,axis=1)\n",
    "%xdel temp4\n",
    "\n",
    "temp4 =  temp.merge(right=veh_tst_df_ext, how='outer')\n",
    "veh_tst_df_ext = temp4.iloc[np.arange(start=n_trn_trail,stop=temp4.shape[0]),:].fillna(0,axis=1)\n",
    "veh_tst_df_ext.index = temp_index_veh_tst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%xdel temp4\n",
    "veh_trn_df_ext.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "veh_tst_df_ext.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if ifWrite:\n",
    "    try:\n",
    "        os.remove('./train/vehicle_train_ext.csv')\n",
    "        os.remove('./test/vehicle_test_ext.csv')\n",
    "    except OSError:\n",
    "        pass\n",
    "    veh_trn_df_ext.to_csv('./train/vehicle_train_ext.csv')\n",
    "    veh_tst_df_ext.to_csv('./test/vehicle_test_ext.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#veh_tst_df_ext.to_csv('./test/vehicle_test_ext.csv')\n",
    "%xdel veh_trn_df_ext\n",
    "%xdel veh_tst_df_ext\n",
    "%xdel temp\n",
    "%xdel temp_index_veh_tst\n",
    "%xdel temp_index_veh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if ifWrite:\n",
    "    conn = sqlite3.connect(\"./train/joint_accident_person_vehicle.db\")\n",
    "    n_batch = 1000\n",
    "\n",
    "    for i in np.arange(np.ceil(n_veh_trn/n_batch).astype(np.int64), dtype=np.int64):\n",
    "        if i == 0:\n",
    "            temp = pd.read_csv('./train/vehicle_train_ext.csv', nrows=n_batch, index_col=0)\n",
    "            temp.to_sql(\"vehicle_trans\", conn, if_exists= \"replace\", index=True)\n",
    "        else:\n",
    "            if i!= np.ceil(n_per_veh/n_batch).astype(np.int64)-1:\n",
    "                temp = pd.read_csv('./train/vehicle_train_ext.csv', \\\n",
    "                               skiprows= range(1, i*n_batch+1),\\\n",
    "                               nrows=n_batch, index_col=0)\n",
    "                temp.to_sql(\"vehicle_trans\", conn, if_exists= \"append\", index=True)\n",
    "            else:\n",
    "                temp = pd.read_csv('./train/vehicle_train_ext.csv', \\\n",
    "                               skiprows= range(1, i*n_batch+1),\\\n",
    "                               index_col=0)\n",
    "                temp.to_sql(\"vehicle_trans\", conn, if_exists= \"append\", index=True)\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#%xdel temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine Accident and Person data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the transformed data from disk. Note that the total size is very large. Need to reset the kernel and release all memory to load the data. Make sure release memory after processing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "acc_trn_df_ext = pd.read_csv('./train/accident_train_ext.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "acc_trn_df_ext.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%reset\n",
    "per_trn_df_ext = pd.read_csv('./train/person_train_ext.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "per_trn_df_ext.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "acc_per_joined_trn_df = pd.merge(acc_trn_df_ext, per_trn_df_ext, on='ID', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "acc_per_joined_trn_df.shape\n",
    "#joined_trn_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The joint data set filled the memory, so fill nan in batch mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_batch = 10000\n",
    "n_acc_per = acc_per_joined_trn_df.shape[0]\n",
    "\n",
    "for i in np.arange(np.ceil(n_acc_per/n_batch).astype(np.int64), dtype=np.int64):\n",
    "    if i != np.ceil(n_acc_per/n_batch).astype(np.int64)-1:\n",
    "        acc_per_joined_trn_df.loc[np.arange(start=i*n_batch, stop=(i+1)*n_batch),:].fillna(0, inplace=True)\n",
    "    else:\n",
    "        acc_per_joined_trn_df.loc[np.arange(start=i*n_batch, stop=n_acc_per),:].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if ifWrite:\n",
    "    acc_per_joined_trn_df.to_csv('./train/joint_accident_person_train_ext.csv')\n",
    "#joined_trn_df.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write to sqlite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if ifWrite:\n",
    "    conn = sqlite3.connect(\"./train/joint_accident_person_vehicle.db\")\n",
    "    n_batch = 10000\n",
    "\n",
    "    for i in np.arange(np.ceil(n_acc_per/n_batch).astype(np.int64), dtype=np.int64):\n",
    "        if i == 0:\n",
    "            acc_per_joined_trn_df.loc[np.arange(start=i*n_batch, stop=(i+1)*n_batch),:]\\\n",
    "                             .to_sql(\"accident_person\", conn, if_exists= \"replace\", index=True)\n",
    "        else:\n",
    "            if i!= np.ceil(n_acc_per/n_batch).astype(np.int64)-1:\n",
    "                acc_per_joined_trn_df.loc[np.arange(start=i*n_batch, stop=(i+1)*n_batch),:]\\\n",
    "                            .to_sql(\"accident_person\", conn, if_exists= \"append\", index=True)\n",
    "            else:\n",
    "                acc_per_joined_trn_df.loc[np.arange(start=i*n_batch, stop=n_acc_per),:].\\\n",
    "                             to_sql(\"accident_person\", conn, if_exists= \"append\", index=True)\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%xdel acc_trn_df_ext\n",
    "%xdel per_trn_df_ext\n",
    "%xdel acc_per_joined_trn_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "acc_tst_df_ext = pd.read_csv('./test/accident_test_ext.csv', index_col=0)\n",
    "acc_tst_df_ext.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "per_tst_df_ext = pd.read_csv('./test/person_test_ext.csv', index_col=0)\n",
    "per_tst_df_ext.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "acc_per_joined_tst_df = pd.merge(acc_tst_df_ext, per_tst_df_ext, on='ID', how='inner')\n",
    "acc_per_joined_tst_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "acc_per_joined_tst_df.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if ifWrite:\n",
    "    acc_per_joined_tst_df.to_csv('./test/joint_accident_person_test_ext.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%xdel acc_tst_df_ext\n",
    "%xdel per_tst_df_ext\n",
    "%xdel acc_per_joined_tst_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine the Person and Vehicle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "per_trn_df_ext = pd.read_csv('./train/person_train_ext.csv', index_col=0)\n",
    "per_trn_df_ext.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "veh_trn_df_ext = pd.read_csv('./train/vehicle_train_ext.csv', index_col=0)\n",
    "veh_trn_df_ext.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "duplicate_list = veh_trn_df_ext.columns[veh_trn_df_ext.columns.isin(per_trn_df_ext.columns)].drop(['ID','VEH_NO'])\n",
    "duplicate_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "veh_trn_df_ext.drop(duplicate_list, axis=1,inplace=True)\n",
    "veh_trn_df_ext.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "per_veh_joined_trn_df = pd.merge(per_trn_df_ext, veh_trn_df_ext, on=['ID','VEH_NO'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "per_veh_joined_trn_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_per_veh = per_veh_joined_trn_df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n_batch = 10000\n",
    "for i in np.arange(np.ceil(n_per_veh/n_batch).astype(np.int64), dtype=np.int64):\n",
    "    if i == 0:\n",
    "        temp = pd.read_csv('./train/joint_person_vehicle_train_ext.csv', nrows=n_batch, index_col=0)\n",
    "        temp.fillna(0, inplace=True)\n",
    "        per_veh_joined_trn_df = temp\n",
    "    else:\n",
    "        temp = pd.read_csv('./train/joint_person_vehicle_train_ext.csv', \\\n",
    "                               skiprows= range(1, i*n_batch+1),\\\n",
    "                               nrows=n_batch, index_col=0)\n",
    "        temp.fillna(0, inplace=True)\n",
    "        per_veh_joined_trn_df = pd.concat([per_veh_joined_trn_df, temp])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Write to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if ifWrite:\n",
    "    per_veh_joined_trn_df.to_csv('./train/joint_person_vehicle_train_ext.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%xdel veh_trn_df_ext\n",
    "%xdel per_trn_df_ext\n",
    "%xdel per_veh_joined_trn_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write to sqlite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ifWrite = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if ifWrite:\n",
    "    conn = sqlite3.connect(\"./train/joint_accident_person_vehicle.db\")\n",
    "    n_batch = 1000\n",
    "\n",
    "    for i in np.arange(np.ceil(n_per_veh/n_batch).astype(np.int64), dtype=np.int64):\n",
    "        if i == 0:\n",
    "            temp = pd.read_csv('./train/joint_person_vehicle_train_ext.csv', nrows=n_batch, index_col=0)\n",
    "            temp.fillna(0, inplace=True)\n",
    "            temp.to_sql(\"person_vehcile\", conn, if_exists= \"replace\", index=True)\n",
    "        else:\n",
    "            if i!= np.ceil(n_per_veh/n_batch).astype(np.int64)-1:\n",
    "                temp = pd.read_csv('./train/joint_person_vehicle_train_ext.csv', \\\n",
    "                               skiprows= range(1, i*n_batch+1),\\\n",
    "                               nrows=n_batch, index_col=0)\n",
    "                temp.fillna(0, inplace=True)\n",
    "                temp.to_sql(\"person_vehcile\", conn, if_exists= \"append\", index=True)\n",
    "            else:\n",
    "                temp = pd.read_csv('./train/joint_person_vehicle_train_ext.csv', \\\n",
    "                               skiprows= range(1, i*n_batch+1),\\\n",
    "                               index_col=0)\n",
    "                temp.fillna(0, inplace=True)\n",
    "                temp.to_sql(\"person_vehcile\", conn, if_exists= \"append\", index=True)\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "per_tst_df_ext = pd.read_csv('./test/person_test_ext.csv', index_col=0)\n",
    "per_tst_df_ext.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "veh_tst_df_ext = pd.read_csv('./test/vehicle_test_ext.csv', index_col=0)\n",
    "veh_tst_df_ext.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "per_veh_joined_tst_df = pd.merge(per_tst_df_ext, veh_tst_df_ext, on=['ID','VEH_NO'], how='left')\n",
    "per_veh_joined_tst_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "per_veh_joined_tst_df.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if ifWrite:\n",
    "    per_veh_joined_tst_df.to_csv('./test/joint_person_vehicle_test_ext.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total number of accident ID included for given (PER_NO, VEH_NO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "mat_perno_vehno = joined_trn_df['ID'].groupby([joined_trn_df['PER_NO'], joined_trn_df['VEH_NO']]).count().unstack()\n",
    "type(mat_perno_vehno)\n",
    "sns.set(style=\"white\", font_scale=1.5)\n",
    "f= plt.figure(figsize=(11, 9))\n",
    "ax = plt.gca()\n",
    "### Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "### Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(mat_perno_vehno.fillna(0,axis=0), cmap=cmap, vmax=np.log10(np.max(np.max(mat_perno_vehno))+1),\n",
    "            square=True, xticklabels=5, yticklabels=5,\n",
    "            linewidths=.5, cbar_kws={\"shrink\": .5}, ax=ax)\n",
    "\n",
    "cax = plt.gcf().axes[-1]\n",
    "cax.tick_params(labelsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "%xdel joined_trn_df\n",
    "joined_trn_df = pd.merge(acc_trn_df_ext, per_trn_df_ext, on='ID', how='inner')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
